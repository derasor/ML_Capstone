{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ref2:\n",
    "https://www.kaggle.com/kmader/nuclei-overview-to-submission\n",
    "'''\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from glob import glob\n",
    "import os\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "dsb_data_dir = os.path.join('..', 'input')\n",
    "stage_label = 'stage1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../input/stage1_train_labels.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c8defee56523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read labels: Load the RLE-encoded output for the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsb_data_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'{}_train_labels.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EncodedPixels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EncodedPixels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deraso/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deraso/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deraso/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deraso/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/deraso/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../input/stage1_train_labels.csv does not exist"
     ]
    }
   ],
   "source": [
    "# Read labels: Load the RLE-encoded output for the training set\n",
    "\n",
    "train_labels = pd.read_csv(os.path.join(dsb_data_dir,'{}_train_labels.csv'.format(stage_label)))\n",
    "train_labels['EncodedPixels'] = train_labels['EncodedPixels'].map(lambda ep: [int(x) for x in ep.split(' ')])\n",
    "train_labels.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0259130e6269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mimg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TrainingSplit'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Stage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_stage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/deraso/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load in all Images\n",
    "Here we load in the images and process the paths so we have the appropriate information for each image\n",
    "'''\n",
    "\n",
    "all_images = glob(os.path.join(dsb_data_dir, 'stage1_*', '*', '*', '*'))\n",
    "img_df = pd.DataFrame({'path': all_images})\n",
    "img_id = lambda in_path: in_path.split('/')[-3]\n",
    "img_type = lambda in_path: in_path.split('/')[-2]\n",
    "img_group = lambda in_path: in_path.split('/')[-4].split('_')[1]\n",
    "img_stage = lambda in_path: in_path.split('/')[-4].split('_')[0]\n",
    "img_df['ImageId'] = img_df['path'].map(img_id)\n",
    "img_df['ImageType'] = img_df['path'].map(img_type)\n",
    "img_df['TrainingSplit'] = img_df['path'].map(img_group)\n",
    "img_df['Stage'] = img_df['path'].map(img_stage)\n",
    "img_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time\\ntrain_df = img_df.query(\\'TrainingSplit==\"train\"\\')\\ntrain_rows = []\\ngroup_cols = [\\'Stage\\', \\'ImageId\\']\\nfor n_group, n_rows in train_df.groupby(group_cols):\\n    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\\n    c_row[\\'masks\\'] = n_rows.query(\\'ImageType == \"masks\"\\')[\\'path\\'].values.tolist()\\n    c_row[\\'images\\'] = n_rows.query(\\'ImageType == \"images\"\\')[\\'path\\'].values.tolist()\\n    train_rows += [c_row]\\ntrain_img_df = pd.DataFrame(train_rows)    \\nIMG_CHANNELS = 3\\ndef read_and_stack(in_img_list):\\n    return np.sum(np.stack([imread(c_img) for c_img in in_img_list], 0), 0)/255.0\\ntrain_img_df[\\'images\\'] = train_img_df[\\'images\\'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\\ntrain_img_df[\\'masks\\'] = train_img_df[\\'masks\\'].map(read_and_stack).map(lambda x: x.astype(int))\\ntrain_img_df.sample(1)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create Training Data\n",
    "Here we make training data and load all the images into the dataframe. \n",
    "We take a simplification here of grouping all the regions together (rather than keeping them distinct).\n",
    "'''\n",
    "\n",
    "'''\n",
    "%%time\n",
    "train_df = img_df.query('TrainingSplit==\"train\"')\n",
    "train_rows = []\n",
    "group_cols = ['Stage', 'ImageId']\n",
    "for n_group, n_rows in train_df.groupby(group_cols):\n",
    "    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\n",
    "    c_row['masks'] = n_rows.query('ImageType == \"masks\"')['path'].values.tolist()\n",
    "    c_row['images'] = n_rows.query('ImageType == \"images\"')['path'].values.tolist()\n",
    "    train_rows += [c_row]\n",
    "train_img_df = pd.DataFrame(train_rows)    \n",
    "IMG_CHANNELS = 3\n",
    "def read_and_stack(in_img_list):\n",
    "    return np.sum(np.stack([imread(c_img) for c_img in in_img_list], 0), 0)/255.0\n",
    "train_img_df['images'] = train_img_df['images'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\n",
    "train_img_df['masks'] = train_img_df['masks'].map(read_and_stack).map(lambda x: x.astype(int))\n",
    "train_img_df.sample(1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_img = 6\\nfig, m_axs = plt.subplots(2, n_img, figsize = (12, 4))\\nfor (_, c_row), (c_im, c_lab) in zip(train_img_df.sample(n_img).iterrows(), \\n                                     m_axs.T):\\n    c_im.imshow(c_row['images'])\\n    c_im.axis('off')\\n    c_im.set_title('Microscope')\\n    \\n    c_lab.imshow(c_row['masks'])\\n    c_lab.axis('off')\\n    c_lab.set_title('Labeled')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Show a few images\n",
    "Here we show a few images of the cells where we see there is a mixture \n",
    "of brightfield and fluorescence which will probably make using \n",
    "a single segmentation algorithm difficult\n",
    "'''\n",
    "\n",
    "'''\n",
    "n_img = 6\n",
    "fig, m_axs = plt.subplots(2, n_img, figsize = (12, 4))\n",
    "for (_, c_row), (c_im, c_lab) in zip(train_img_df.sample(n_img).iterrows(), \n",
    "                                     m_axs.T):\n",
    "    c_im.imshow(c_row['images'])\n",
    "    c_im.axis('off')\n",
    "    c_im.set_title('Microscope')\n",
    "    \n",
    "    c_lab.imshow(c_row['masks'])\n",
    "    c_lab.axis('off')\n",
    "    c_lab.set_title('Labeled')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_img_df['Red'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,0]))\\ntrain_img_df['Green'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,1]))\\ntrain_img_df['Blue'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,2]))\\ntrain_img_df['Gray'] = train_img_df['images'].map(lambda x: np.mean(x))\\ntrain_img_df['Red-Blue'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,0]-x[:,:,2]))\\nsns.pairplot(train_img_df[['Gray', 'Red', 'Green', 'Blue', 'Red-Blue']])\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Look at the intensity distribution\n",
    "Here we look briefly at the distribution of intensity and see a few groups forming, \n",
    "they should probably be handled separately.\n",
    "'''\n",
    "\n",
    "'''\n",
    "train_img_df['Red'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,0]))\n",
    "train_img_df['Green'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,1]))\n",
    "train_img_df['Blue'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,2]))\n",
    "train_img_df['Gray'] = train_img_df['images'].map(lambda x: np.mean(x))\n",
    "train_img_df['Red-Blue'] = train_img_df['images'].map(lambda x: np.mean(x[:,:,0]-x[:,:,2]))\n",
    "sns.pairplot(train_img_df[['Gray', 'Red', 'Green', 'Blue', 'Red-Blue']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_img_df['images'].map(lambda x: x.shape).value_counts()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Check Dimensions\n",
    "Here we show the dimensions of the data to see the variety in the input images\n",
    "'''\n",
    "\n",
    "'''\n",
    "train_img_df['images'].map(lambda x: x.shape).value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.models import Sequential\\nfrom keras.layers import BatchNormalization, Conv2D, UpSampling2D, Lambda\\nsimple_cnn = Sequential()\\nsimple_cnn.add(BatchNormalization(input_shape = (None, None, IMG_CHANNELS), \\n                                  name = 'NormalizeInput'))\\nsimple_cnn.add(Conv2D(8, kernel_size = (3,3), padding = 'same'))\\nsimple_cnn.add(Conv2D(8, kernel_size = (3,3), padding = 'same'))\\n# use dilations to get a slightly larger field of view\\nsimple_cnn.add(Conv2D(16, kernel_size = (3,3), dilation_rate = 2, padding = 'same'))\\nsimple_cnn.add(Conv2D(16, kernel_size = (3,3), dilation_rate = 2, padding = 'same'))\\nsimple_cnn.add(Conv2D(32, kernel_size = (3,3), dilation_rate = 3, padding = 'same'))\\n\\n# the final processing\\nsimple_cnn.add(Conv2D(16, kernel_size = (1,1), padding = 'same'))\\nsimple_cnn.add(Conv2D(1, kernel_size = (1,1), padding = 'same', activation = 'sigmoid'))\\nsimple_cnn.summary()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Making a simple CNN\n",
    "Here we make a very simple CNN just to get a quick idea of how well it works. \n",
    "For this we use a batch normalization to normalize the inputs. \n",
    "We cheat a bit with the padding to keep problems simple.\n",
    "'''\n",
    "\n",
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv2D, UpSampling2D, Lambda\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(BatchNormalization(input_shape = (None, None, IMG_CHANNELS), \n",
    "                                  name = 'NormalizeInput'))\n",
    "simple_cnn.add(Conv2D(8, kernel_size = (3,3), padding = 'same'))\n",
    "simple_cnn.add(Conv2D(8, kernel_size = (3,3), padding = 'same'))\n",
    "# use dilations to get a slightly larger field of view\n",
    "simple_cnn.add(Conv2D(16, kernel_size = (3,3), dilation_rate = 2, padding = 'same'))\n",
    "simple_cnn.add(Conv2D(16, kernel_size = (3,3), dilation_rate = 2, padding = 'same'))\n",
    "simple_cnn.add(Conv2D(32, kernel_size = (3,3), dilation_rate = 3, padding = 'same'))\n",
    "\n",
    "# the final processing\n",
    "simple_cnn.add(Conv2D(16, kernel_size = (1,1), padding = 'same'))\n",
    "simple_cnn.add(Conv2D(1, kernel_size = (1,1), padding = 'same', activation = 'sigmoid'))\n",
    "simple_cnn.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras import backend as K\\nsmooth = 1.\\ndef dice_coef(y_true, y_pred):\\n    y_true_f = K.flatten(y_true)\\n    y_pred_f = K.flatten(y_pred)\\n    intersection = K.sum(y_true_f * y_pred_f)\\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\\ndef dice_coef_loss(y_true, y_pred):\\n    return -dice_coef(y_true, y_pred)\\nsimple_cnn.compile(optimizer = 'adam', \\n                   loss = dice_coef_loss, \\n                   metrics = [dice_coef, 'acc', 'mse'])\\n\\n\\ndef simple_gen():\\n    while True:\\n        for _, c_row in train_img_df.iterrows():\\n            yield np.expand_dims(c_row['images'],0), np.expand_dims(np.expand_dims(c_row['masks'],-1),0)\\n\\nsimple_cnn.fit_generator(simple_gen(), \\n                         steps_per_epoch=train_img_df.shape[0],\\n                         epochs = 3)\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras import backend as K\n",
    "smooth = 1.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "simple_cnn.compile(optimizer = 'adam', \n",
    "                   loss = dice_coef_loss, \n",
    "                   metrics = [dice_coef, 'acc', 'mse'])\n",
    "\n",
    "\n",
    "def simple_gen():\n",
    "    while True:\n",
    "        for _, c_row in train_img_df.iterrows():\n",
    "            yield np.expand_dims(c_row['images'],0), np.expand_dims(np.expand_dims(c_row['masks'],-1),0)\n",
    "\n",
    "simple_cnn.fit_generator(simple_gen(), \n",
    "                         steps_per_epoch=train_img_df.shape[0],\n",
    "                         epochs = 3)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time\\ntest_df = img_df.query(\\'TrainingSplit==\"test\"\\')\\ntest_rows = []\\ngroup_cols = [\\'Stage\\', \\'ImageId\\']\\nfor n_group, n_rows in test_df.groupby(group_cols):\\n    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\\n    c_row[\\'images\\'] = n_rows.query(\\'ImageType == \"images\"\\')[\\'path\\'].values.tolist()\\n    test_rows += [c_row]\\ntest_img_df = pd.DataFrame(test_rows)    \\n\\ntest_img_df[\\'images\\'] = test_img_df[\\'images\\'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\\nprint(test_img_df.shape[0], \\'images to process\\')\\ntest_img_df.sample(1)\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "test_df = img_df.query('TrainingSplit==\"test\"')\n",
    "test_rows = []\n",
    "group_cols = ['Stage', 'ImageId']\n",
    "for n_group, n_rows in test_df.groupby(group_cols):\n",
    "    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\n",
    "    c_row['images'] = n_rows.query('ImageType == \"images\"')['path'].values.tolist()\n",
    "    test_rows += [c_row]\n",
    "test_img_df = pd.DataFrame(test_rows)    \n",
    "\n",
    "test_img_df['images'] = test_img_df['images'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\n",
    "print(test_img_df.shape[0], 'images to process')\n",
    "test_img_df.sample(1)\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\ntest_img_df['masks'] = test_img_df['images'].map(lambda x: simple_cnn.predict(np.expand_dims(x, 0))[0, :, :, 0])\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "test_img_df['masks'] = test_img_df['images'].map(lambda x: simple_cnn.predict(np.expand_dims(x, 0))[0, :, :, 0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_img = 3\\nfrom skimage.morphology import closing, opening, disk\\ndef clean_img(x):\\n    return opening(closing(x, disk(1)), disk(3))\\nfig, m_axs = plt.subplots(3, n_img, figsize = (12, 6))\\nfor (_, d_row), (c_im, c_lab, c_clean) in zip(test_img_df.sample(n_img).iterrows(), \\n                                     m_axs):\\n    c_im.imshow(d_row['images'])\\n    c_im.axis('off')\\n    c_im.set_title('Microscope')\\n    \\n    c_lab.imshow(d_row['masks'])\\n    c_lab.axis('off')\\n    c_lab.set_title('Predicted')\\n    \\n    c_clean.imshow(clean_img(d_row['masks']))\\n    c_clean.axis('off')\\n    c_clean.set_title('Clean')\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Show a few predictions\n",
    "'''\n",
    "\n",
    "'''\n",
    "n_img = 3\n",
    "from skimage.morphology import closing, opening, disk\n",
    "def clean_img(x):\n",
    "    return opening(closing(x, disk(1)), disk(3))\n",
    "fig, m_axs = plt.subplots(3, n_img, figsize = (12, 6))\n",
    "for (_, d_row), (c_im, c_lab, c_clean) in zip(test_img_df.sample(n_img).iterrows(), \n",
    "                                     m_axs):\n",
    "    c_im.imshow(d_row['images'])\n",
    "    c_im.axis('off')\n",
    "    c_im.set_title('Microscope')\n",
    "    \n",
    "    c_lab.imshow(d_row['masks'])\n",
    "    c_lab.axis('off')\n",
    "    c_lab.set_title('Predicted')\n",
    "    \n",
    "    c_clean.imshow(clean_img(d_row['masks']))\n",
    "    c_clean.axis('off')\n",
    "    c_clean.set_title('Clean')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Check RLE](https://www.kaggle.com/rakhlin/fast-run-length-encoding-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-13-a04afb17e987>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a04afb17e987>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    x: numpy array of shape (height, width), 1 - mask, 0 - background\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from skimage.morphology import label # label regions\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def prob_to_rles(x, cut_off = 0.5):\n",
    "    lab_img = label(x>cut_off)\n",
    "    if lab_img.max()<1:\n",
    "        lab_img[0,0] = 1 # ensure at least one prediction per image\n",
    "    for i in range(1, lab_img.max()+1):\n",
    "        yield rle_encoding(lab_img==i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n_, train_rle_row = next(train_img_df.tail(5).iterrows()) \\ntrain_row_rles = list(prob_to_rles(train_rle_row['masks']))\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the RLEs for a Train Image\n",
    "\n",
    "'''\n",
    "_, train_rle_row = next(train_img_df.tail(5).iterrows()) \n",
    "train_row_rles = list(prob_to_rles(train_rle_row['masks']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntl_rles = train_labels.query(\\'ImageId==\"{ImageId}\"\\'.format(**train_rle_row))[\\'EncodedPixels\\']\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the RLEs from the CSV\n",
    "\n",
    "'''\n",
    "tl_rles = train_labels.query('ImageId==\"{ImageId}\"'.format(**train_rle_row))['EncodedPixels']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmatch, mismatch = 0, 0\\nfor img_rle, train_rle in zip(sorted(train_row_rles, key = lambda x: x[0]), \\n                             sorted(tl_rles, key = lambda x: x[0])):\\n    for i_x, i_y in zip(img_rle, train_rle):\\n        if i_x == i_y:\\n            match += 1\\n        else:\\n            mismatch += 1\\nprint('Matches: %d, Mismatches: %d, Accuracy: %2.1f%%' % (match, mismatch, 100.0*match/(match+mismatch)))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "match, mismatch = 0, 0\n",
    "for img_rle, train_rle in zip(sorted(train_row_rles, key = lambda x: x[0]), \n",
    "                             sorted(tl_rles, key = lambda x: x[0])):\n",
    "    for i_x, i_y in zip(img_rle, train_rle):\n",
    "        if i_x == i_y:\n",
    "            match += 1\n",
    "        else:\n",
    "            mismatch += 1\n",
    "print('Matches: %d, Mismatches: %d, Accuracy: %2.1f%%' % (match, mismatch, 100.0*match/(match+mismatch)))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nout_pred_list = []\\nfor _, c_row in test_img_df.iterrows():\\n    for c_rle in c_row['rles']:\\n        out_pred_list+=[dict(ImageId=c_row['ImageId'], \\n                             EncodedPixels = ' '.join(np.array(c_rle).astype(str)))]\\nout_pred_df = pd.DataFrame(out_pred_list)\\nprint(out_pred_df.shape[0], 'regions found for', test_img_df.shape[0], 'images')\\nout_pred_df.sample(3)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Calculate RLE for all the masks\n",
    "'''\n",
    "\n",
    "'''\n",
    "test_img_df['rles'] = test_img_df['masks'].map(clean_img).map(lambda x: list(prob_to_rles(x)))\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "out_pred_list = []\n",
    "for _, c_row in test_img_df.iterrows():\n",
    "    for c_rle in c_row['rles']:\n",
    "        out_pred_list+=[dict(ImageId=c_row['ImageId'], \n",
    "                             EncodedPixels = ' '.join(np.array(c_rle).astype(str)))]\n",
    "out_pred_df = pd.DataFrame(out_pred_list)\n",
    "print(out_pred_df.shape[0], 'regions found for', test_img_df.shape[0], 'images')\n",
    "out_pred_df.sample(3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nout_pred_df[['ImageId', 'EncodedPixels']].to_csv('predictions.csv', index = False)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "out_pred_df[['ImageId', 'EncodedPixels']].to_csv('predictions.csv', index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
