{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal\n",
    "---------------\n",
    "David Eraso  \n",
    "February 23th, 2018\n",
    "\n",
    "---------------\n",
    "\n",
    "## Find the nuclei in divergent images to advance medical discovery\n",
    "\n",
    "Kaggle/Booz Allen Hamilton competition [2018 Data Scince Bowl](https://www.kaggle.com/c/data-science-bowl-2018)\n",
    "\n",
    "\n",
    "### Domain Background\n",
    "\n",
    "Identifying the cells’ nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work. By automating nucleus detection, research could unlock cures faster by allowing more efficient drug testing.\n",
    "\n",
    "I am personally interested in using Machine Learning techniques in topics related to health.  \n",
    "\n",
    "Related Videos:\n",
    "\n",
    "[2018 Data Science Bowl](https://www.youtube.com/watch?v=eHwkfhmJexs&amp;feature=youtu.be)  \n",
    "[Technical Overview](https://datasciencebowl.com/2018dsbtutorial/)\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The objective is to \"create a computer model that can identify a range of nuclei across varied conditions\". Since this is an object detection task, the evaluation metric that will be used is the mean average precision (mAP) at different [Intersection over Union](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) (IoU) thresholds. Details will be explored in more detail on the 'Evaluation Metrics' section.\n",
    "\n",
    "\n",
    "### Datasets and Inputs\n",
    "\n",
    "Datasets are obtained from the [Data Web Page](https://www.kaggle.com/c/data-science-bowl-2018/data) of the competition. From this reference:\n",
    "\n",
    "_This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations._\n",
    "\n",
    "_Each image is represented by an associated ImageId. Files belonging to an image are contained in a folder with this ImageId. Within this folder are two subfolders:_\n",
    "\n",
    "* `images` contains the image file.\n",
    "* `masks` contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).\n",
    "\n",
    "_The second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details._\n",
    "\n",
    "_As with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set._\n",
    "\n",
    "_File descriptions:_\n",
    "\n",
    "`/stage1_train/*` - training set images (images and annotated masks)  \n",
    "`/stage1_test/*` - stage 1 test set images (images only, you are predicting the masks)  \n",
    "`/stage2_test/*` (released later) - stage 2 test set images (images only, you are predicting the masks)  \n",
    "`stage1_sample_submission.csv` - a submission file containing the ImageIds for which you must predict during stage 1  \n",
    "`stage2_sample_submission.csv` (released later) - a submission file containing the ImageIds for which you must predict during stage 2  \n",
    "`stage1_train_labels.csv` - a file showing the run-length encoded representation of the training images. This is provided as a convenience and is redundant with the mask image files.  \n",
    "\n",
    "\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "I'm looking to develop an architecture capable of achieving a score good enough to make it into the first 100 places in the leaderboard. By February 19, the 100th place had a score of 0.395. The first place, not prize eligible, is a score of 0.634.\n",
    "\n",
    "This is my first Kaggle competition and I don't want to be overambitious. The evaluation metric used in the leaderboard is a mAP described in the 'Evaluation Metric' section.\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    "My Benchmark model is the kernel [`U-Net starter - LB 0.277`](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277), by Kjetil Åmdal-SævikKeras. Beyond implementing the basic setup of getting the data as well as encoding results for submission, Kjetil builds a  U-Net model, '_loosely based on [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) and very similar to [this repo](https://github.com/jocicmarko/ultrasound-nerve-segmentation) from the Kaggle Ultrasound Nerve Segmentation competition._'\n",
    "\n",
    "<img src=\"U-Net.png\">\n",
    " \n",
    "This kernel is a good starting point with a score of 0.277.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "[From the 'Evaluation' link on the competition webpage:](https://www.kaggle.com/c/data-science-bowl-2018#evaluation)\n",
    "\n",
    "_This competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:_\n",
    "\n",
    "$$IoU(A,B) = \\frac{A \\cap B}{ A \\cup B}$$\n",
    "\n",
    "_The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05._\n",
    "\n",
    "_In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5._\n",
    "\n",
    "_At each threshold value t, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:_\n",
    "\n",
    "$$\\frac{TP(t)}{TP(t)+FP(t)+FN(t)}$$\n",
    "\n",
    "_A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:_\n",
    "\n",
    "$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}$$\n",
    "\n",
    "_Lastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset._\n",
    "\n",
    "\n",
    "### Project Design\n",
    "\n",
    "References:  \n",
    "[A Brief History CNNs in Image Segmentation: From R-CNN to Mask R-CNN](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4)  \n",
    "[Image Segmentation](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_segmentation.html)  \n",
    "[A 2017 Guide to Semantic Segmentation with Deep Learning](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review)  \n",
    "\n",
    "Convolutional Networks have been used succesfully for both image classification and image recognition. The benchmark model mentioned above is based on a Convolutional Network designed specifically for Biomedical Image Segmentation, the U-Net. \n",
    "\n",
    "This is clearly an Image Segmentation problem. The images have overlapping objects, nuclei, and we want to correctly identify the object and its boundaries. Since we are interested in masking, we need pixel level segmentation. Pixel segmentation was explored at Facebook AI by Kaiming He and a team of researchers, including Girshick, using an architecture known as [Mask R-CNN](https://arxiv.org/abs/1703.06870). Basically they used an existing architecture called [Faster R-CNN](https://arxiv.org/abs/1506.01497) and replaced the last layers that gave the classification and boundary box with a Fully Convolutional Network (FCN) to generate the mask:\n",
    "\n",
    "From [Mask R-CNN](https://arxiv.org/abs/1703.06870):\n",
    "\n",
    "<img src=\"maskRCNN.png\">\n",
    "\n",
    "The mask is a binary classifier that defines whether or not a given pixel is part of an object. In this case, we are interested in whether the pixel is part of a given nucleus or not (background or other nuclei). The FCN  is a normal CNN, where the last fully connected layer is substituted by another convolution layer with a large \"receptive field\". [Here](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_segmentation.html) there is an example of how to convert a normal CNN used for classification, ie: Alexnet to a FCN used for segmentation. \n",
    "\n",
    "The U-Net also uses the Transposed Convoultional Layer idea, also called \"Deconvolution\", to scale up the scale down effect done on the first layers. This is the \"Deconvnet\" architecture that according to the reference _it takes 6 days to train on a TitanX_ and needs to be trained in 2-stages (Single objects centered, and fine tuning with difficult examples):\n",
    "\n",
    "<img src=\"deconvnet.png\">\n",
    "\n",
    "There are several architectures that have been developed to tackle this kind of problem. Understanding the image at pixel level is also called Semantic Segmentation. A list of architectures for Semantic Segmentation can be found [here](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review). They are:\n",
    "\n",
    "1. Fully Convolutional Networks for Semantic Segmentation\n",
    "2. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation\n",
    "3. Multi-Scale Context Aggregation by Dilated Convolutions\n",
    "4. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\n",
    "5. RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation\n",
    "6. Pyramid Scene Parsing Network\n",
    "7. Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network\n",
    "8. Rethinking Atrous Convolution for Semantic Image Segmentation\n",
    "\n",
    "#### The Steps in This Project:\n",
    "\n",
    "I propose to explain roughly the architectures mentioned above, and why each one is or isn't a relevant candidate for the specific problem at hand. I will then implement three architectures based on the ones that pose the most promise. Every experiment will have a score. The best alternative can then be tuned further. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
